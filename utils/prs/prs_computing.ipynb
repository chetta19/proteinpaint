{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64de9cda",
   "metadata": {},
   "source": [
    "# <center>PRS computing</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7b09b",
   "metadata": {},
   "source": [
    "This page documents the PRS computing for PGS catalog. It contain 3 steps: <br>1. PGScatalog download <br>2. Computing <br>3. File production<br><font color = red>You need to set up the working directory and provide the path for some data required for PRS computing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e199a",
   "metadata": {},
   "source": [
    "## working directory and required file setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f241d",
   "metadata": {},
   "source": [
    "1. Working Directory <br>&nbsp;&nbsp;&nbsp;&nbsp;PGS score files, PRS computing results and intermediate files will be generated under working directory <br> &nbsp;&nbsp;&nbsp;&nbsp; pgsJsonFile(prs/pgs_scores_data.json) is downloaded from https://www.pgscatalog.org/browse/scores/ <br> 2. PRS Compute <br>&nbsp;&nbsp;&nbsp;&nbsp;SJLIFE_CCSS_varfile is the compressed and indexed SJLIFE_CCSS variants <br>&nbsp;&nbsp;&nbsp;&nbsp;sample list file based on ethinicity under working directory ( sample.asa; sample.ceu; sample.yri) <br>&nbsp;&nbsp;&nbsp;&nbsp;pre-computed hwe test results <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/research/rgs01/resgen/legacy/gb_customTracks/tp/jwang/TASK/survivorship/HWE/combined/forprs/phwe.asa.gz<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/research/rgs01/resgen/legacy/gb_customTracks/tp/jwang/TASK/survivorship/HWE/combined/forprs/phwe.ceu.gz<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/research/rgs01/resgen/legacy/gb_customTracks/tp/jwang/TASK/survivorship/HWE/combined/forprs/phwe.yri.gz<br>3. File production <br>&nbsp;&nbsp;&nbsp;&nbsp;updateTime: The date when PRS was computed<br>&nbsp;&nbsp;&nbsp;&nbsp;pgs_all_metadata.xlsx Download from https://www.pgscatalog.org/downloads/<br>&nbsp;&nbsp;&nbsp;&nbsp;pgs_traits_data.json Download from https://www.pgscatalog.org/browse/traits/<br>&nbsp;&nbsp;&nbsp;&nbsp;pathToPRS: relative path of working directory to tp(jwang/TASK/prs/prsscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06012bf5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working directory and required file setup Done!\n"
     ]
    }
   ],
   "source": [
    "import re,os\n",
    "\n",
    "workingDir = '/research/rgs01/resgen/legacy/gb_customTracks/tp/jwang/TASK/prs'\n",
    "\n",
    "#####download\n",
    "# pgs_scores_data.json downloaded from https://www.pgscatalog.org/browse/scores/\n",
    "pgsJsonFile = os.path.join(workingDir,'pgs_scores_data.json')\n",
    "\n",
    "\n",
    "### PRS Compute\n",
    "#sampleID2IntegerID\n",
    "def get_id2name(id2nameFile):\n",
    "    id2name = {}\n",
    "    fh = open(id2nameFile)\n",
    "    for line in fh:\n",
    "        l = line.strip().split('\\t')\n",
    "        id2name[l[1]] = l[0]\n",
    "    fh.close()\n",
    "    return id2name\n",
    "#SJLIFE_CCSS_varfile\n",
    "SJLIFE_CCSS_varfile = '/research/rgs01/resgen/legacy/gb_customTracks/tp/jwang/TASK/prs/SJLIFE.CCSS.variants.gz'\n",
    "sampleIDmap = '/research/rgs01/resgen/legacy/gb_customTracks/tp/files/hg38/sjlife/clinical/samples.idmap'\n",
    "#sample list file based on ethinicity\n",
    "#ASA: sample.asa; CEU: sample.ceu; YRI: sample.yri (under working directory)\n",
    "sampleFile = {p:os.path.join(workingDir,'sample.'+p) for p in ['asa','ceu','yri']}\n",
    "#sample list file with integer ID\n",
    "id2name = get_id2name(sampleIDmap)\n",
    "for p in sampleFile:\n",
    "    samples = [x.strip() for x in open(sampleFile[p])]\n",
    "    idsamples = [id2name[x] for x in samples]\n",
    "    sample_file = sampleFile[p]+'.intid'\n",
    "    out = open(sample_file,'w')\n",
    "    for intsam in idsamples:\n",
    "        out.write(intsam+'\\n')\n",
    "    out.close()\n",
    "\n",
    "#pre-computed hwe\n",
    "hwedir = '/research/rgs01/resgen/legacy/gb_customTracks/tp/jwang/TASK/survivorship/HWE/combined/forprs'\n",
    "hweTest = {p:os.path.join(hwedir,'phwe.'+p+'.gz') for p in ['asa','ceu','yri']}\n",
    "\n",
    "### File production\n",
    "#The following files are required (under working directory)\n",
    "updateTime = '11/2022'\n",
    "#pgs_all_metadata.xlsx Download from https://www.pgscatalog.org/downloads/\n",
    "#pgs_traits_data.json Download from https://www.pgscatalog.org/browse/traits/\n",
    "pgsMetadata = os.path.join(workingDir,'pgs_all_metadata.xlsx')\n",
    "pgsTraits = os.path.join(workingDir,'pgs_traits_data.json')\n",
    "#relative path to prs scores under tp ('jwang/TASK/prs/prsscore')\n",
    "pathToPRS = 'jwang/TASK/prs'\n",
    "tracktable = os.path.join(workingDir,'track_table.prs')\n",
    "print('working directory and required file setup Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326c17f",
   "metadata": {},
   "source": [
    "## 1. PGScatalog download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc318b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPCF: WARNING! No Memory was requested!\n",
      "      A default memory request of 2.50 GB has been placed for this job\n",
      "      The job will be killed if   2.50 GB of memory is used\n",
      "Job <173141494> is submitted to queue <standard>.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "if not os.path.isfile('pgsdownload.py'):\n",
    "    print('Please run the \"Tools used for prs computing\" below to generate pgsdownload.py')\n",
    "    sys.exit(1)\n",
    "\n",
    "!bsub -q standard python3 pgsdownload.py $pgsJsonFile $workingDir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7ae8e",
   "metadata": {},
   "source": [
    "## 2. PRS Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34861dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job <173256429> is submitted to queue <standard>.\n",
      "Job <173256430> is submitted to queue <standard>.\n",
      "Job <173256431> is submitted to queue <standard>.\n",
      "Job <173256432> is submitted to queue <standard>.\n",
      "Job <173256433> is submitted to queue <standard>.\n",
      "Job <173256434> is submitted to queue <standard>.\n",
      "Job <173256435> is submitted to queue <standard>.\n",
      "Job <173256436> is submitted to queue <standard>.\n",
      "Job <173256437> is submitted to queue <standard>.\n",
      "Job <173256438> is submitted to queue <standard>.\n",
      "Job <173256439> is submitted to queue <standard>.\n",
      "Job <173256440> is submitted to queue <standard>.\n",
      "PRS computing Job submmited!\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "if not os.path.isfile('prs_compute.py'):\n",
    "    print('Please run the \"Tools used for prs computing\" below to generate prs_compute.py')\n",
    "    sys.exit(1)\n",
    "\n",
    "allPGSID = [x for x in os.listdir(workingDir) if os.path.isdir(os.path.join(workingDir,x)) and x.startswith('PGS')]\n",
    "population = ['asa','ceu','yri']\n",
    "for p in population:\n",
    "    for pgs in allPGSID:\n",
    "        scorefile = os.path.join(*[workingDir,pgs,pgs+'_hmPOS_GRCh38.txt.gz'])\n",
    "        if not os.path.isfile(scorefile):\n",
    "            print('Score file was not downloaded successfully: '+pgs)\n",
    "            continue\n",
    "        outprs = os.path.join(*[workingDir,pgs,pgs+'.'+p+'.prs'])\n",
    "        outstat = os.path.join(*[workingDir,pgs,pgs+'.'+p+'.stat'])\n",
    "        outmafprs = os.path.join(*[workingDir,pgs,pgs+'.'+p+'.maf'+'.prs'])\n",
    "        outmafstat = os.path.join(*[workingDir,pgs,pgs+'.'+p+'.maf'+'.stat'])\n",
    "        commandLine = 'bsub -q standard -M 3000 '\n",
    "        commandLine += '-eo '+ os.path.join(*[workingDir,pgs,pgs+'.'+p+'.elog'])\n",
    "        commandLine += ' -oo '+ os.path.join(*[workingDir,pgs,pgs+'.'+p+'.log'])\n",
    "        commandLine += ' python3 prs_compute.py'\n",
    "        commandLine +=  ' -s '+ scorefile\n",
    "        commandLine +=  ' -d '+ os.path.join(workingDir,pgs)\n",
    "        commandLine +=  ' --sample '+ sampleFile[p] \n",
    "        commandLine +=  ' --hwe '+ hweTest[p]\n",
    "        commandLine +=  ' --ancestry '+ p\n",
    "        commandLine += ' -v ' + SJLIFE_CCSS_varfile\n",
    "        commandLine_maf = commandLine\n",
    "        commandLine +=  ' -o ' + outprs\n",
    "        commandLine +=  ' --statout '+ outstat\n",
    "        if p == 'asa':\n",
    "            commandLine +=  ' --matchout'\n",
    "        os.system(commandLine)\n",
    "        #maf filtered\n",
    "        commandLine_maf += ' -o ' + outmafprs\n",
    "        commandLine_maf += ' --statout '+ outmafstat\n",
    "        commandLine_maf +=  ' --maf'\n",
    "        os.system(commandLine_maf)\n",
    "print('PRS computing Job submmited!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d180e9",
   "metadata": {},
   "source": [
    "## 3. File production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a43b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job <173252812> is submitted to queue <standard>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPCF: WARNING! No Memory was requested!\n",
      "      A default memory request of 2.50 GB has been placed for this job\n",
      "      The job will be killed if   2.50 GB of memory is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys,os\n",
    "import re\n",
    "import subprocess as sp\n",
    "\n",
    "if not os.path.isfile('gentracktable.py'):\n",
    "    print('Please run \"Tools used for prs computing\" below to generate gentracktable.py')\n",
    "    sys.exit(1)\n",
    "if not os.path.isfile('make_prs_db.bundle.js'):\n",
    "    print('make_prs_db.bundle.js is not available!')\n",
    "    sys.exit(1)\n",
    "    \n",
    "#Generate track table\n",
    "log_trackTable = os.path.join(workingDir,'tracktable.log')\n",
    "elog_trackTable = os.path.join(workingDir,'tracktable.elog')\n",
    "command_trackTable = 'bsub -q standard'\n",
    "command_trackTable += ' -eo ' + elog_trackTable\n",
    "command_trackTable += ' -oo ' + log_trackTable\n",
    "command_trackTable += ' python3 gentracktable.py'\n",
    "command_trackTable += ' -m ' + pgsMetadata\n",
    "command_trackTable += ' -t ' + pgsTraits\n",
    "command_trackTable += ' -w ' + workingDir\n",
    "command_trackTable += ' -o ' + tracktable\n",
    "command_trackTable += ' -p ' + pathToPRS\n",
    "jobRt = sp.run(command_trackTable, shell=True, stdout=sp.PIPE).stdout.decode('utf-8')\n",
    "jobid = re.search('<(\\d+?)>',jobRt).group(1)\n",
    "\n",
    "#make prs db (tp/files/hg38/sjlife/clinical/PRS/)\n",
    "log_prsdb = os.path.join(workingDir,'prsdb.log')\n",
    "elog_prsdb = os.path.join(workingDir,'prsdb.elog')\n",
    "command_prsdb = 'bsub -q standard -M 20000 -w \"done('+jobid+')\"'\n",
    "command_prsdb += ' -eo ' + elog_prsdb\n",
    "command_prsdb += ' -oo ' + log_prsdb\n",
    "command_prsdb += ' node --max-old-space-size=20480 make_prs_db.bundle.js ' + tracktable + ' ' + updateTime\n",
    "os.system(command_prsdb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01111a35",
   "metadata": {},
   "source": [
    "## Tools used for prs computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c54c880",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pgsdownload.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pgsdownload.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "\n",
    "def EXTPGS(jsfile):\n",
    "    with open(jsfile) as input:\n",
    "        pgsJS = json.load(input)\n",
    "    PGS = []\n",
    "    for p in pgsJS['data']:\n",
    "        PGS.append(re.search('PGS\\d+',p['Polygenic Score ID & Name']).group())\n",
    "    return PGS\n",
    "\n",
    "def GETURL(pgs):\n",
    "    return 'https://ftp.ebi.ac.uk/pub/databases/spot/pgs/scores/'+pgs+'/ScoringFiles/Harmonized/'+pgs+'_hmPOS_GRCh38.txt.gz'\n",
    "\n",
    "\n",
    "PGSID = EXTPGS(sys.argv[1])\n",
    "wd = sys.argv[2]\n",
    "\n",
    "for pid in PGSID:\n",
    "    url = GETURL(pid)\n",
    "    pidDir = os.path.join(wd,pid)\n",
    "    if not os.path.isdir(pidDir):\n",
    "        os.system('mkdir '+pidDir)\n",
    "    command_download = 'LSB_JOB_REPORT_MAIL=N bsub -q standard'\n",
    "    command_download += ' -oo ' + os.path.join(pidDir,pid+'.download.log')\n",
    "    command_download += ' -eo ' + os.path.join(pidDir,pid+'.download.elog')\n",
    "    command_download += ' wget '+url+' -O '+os.path.join(pidDir,pid+'_hmPOS_GRCh38.txt.gz')\n",
    "    os.system(command_download)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282021a1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prs_compute.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prs_compute.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "####################################################\n",
    "#\n",
    "# Computing PRS Score                              #\n",
    "#\n",
    "####################################################\n",
    "\n",
    "script_description=\"\"\"\n",
    "computing PRS score\n",
    "inputfile: \n",
    "           pgs score file from PGS catalog (harmonized hg38)\n",
    "           working directory\n",
    "outputfile:\n",
    "           \n",
    "                        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Initial QC of variant data\n",
    "# functions for Initial QC of variant data \n",
    "##########################################\n",
    "\n",
    "#check strand-ambiguous SNPs\n",
    "def CK_AMB_SNP(a,b):\n",
    "    a = a.upper()\n",
    "    b = b.upper()\n",
    "    AMBSNPS = {'G':'C','C':'G','A':'T','T':'A'}\n",
    "    if a in AMBSNPS and AMBSNPS[a] == b:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def QCVAR(in_f,out_file):\n",
    "\tglobal strandAmbiguousVar,dupVar,sexChromosomeVar\n",
    "\trequired_col = ['chr_name','chr_position','effect_allele','reference_allele','effect_weight']\n",
    "\tfileHeader = []\n",
    "\tfh = gzip.open(in_f)\n",
    "\tout = open(out_file+'.unsort','w')\n",
    "\tout.write('\\t'.join(required_col)+'\\n')\n",
    "\tSEARCHH = ['hm_chr','hm_pos','effect_allele'] #extract columns in this list\n",
    "\tSEARCHD = {}\n",
    "\tuniq_var = set()\n",
    "\tfor line in fh:\n",
    "\t\tline = line.decode('utf-8')\n",
    "\t\tif line.startswith(\"#\"):\n",
    "\t\t\tcontinue\n",
    "\t\telif line.startswith('chr_name') or line.startswith('rsID'):\n",
    "\t\t\tfileHeader = line.strip().split('\\t')\n",
    "\t\t\tif not 'other_allele' in fileHeader:\n",
    "\t\t\t\tSEARCHH.extend(['hm_inferOtherAllele','effect_weight'])\n",
    "\t\t\telse:\n",
    "\t\t\t\tSEARCHH.extend(['other_allele','effect_weight'])\n",
    "\t\t\tfor e in SEARCHH:\n",
    "\t\t\t\tSEARCHD[e] = fileHeader.index(e)\n",
    "\t\t\tcontinue\n",
    "\t\tl = line.replace('\\n','').split('\\t')\n",
    "\t\tif not l[SEARCHD['hm_pos']] or not l[SEARCHD['hm_chr']]:\n",
    "\t\t\tcontinue\n",
    "\t\tif not re.search('^[0-9]',l[SEARCHD['hm_chr']]):\n",
    "\t\t\tsexChromosomeVar += 1 #remove non-autochromosome variant\n",
    "\t\t\tcontinue\n",
    "\t\tOUTL = [l[SEARCHD[e]] for e in SEARCHH]\n",
    "\t\tother_alleles = OUTL[3].split('/')\n",
    "\t\teffalle = OUTL[2]\n",
    "\t\tfor e in other_alleles:\n",
    "\t\t\tvar = '.'.join(OUTL[0:2]+[effalle,e])\n",
    "\t\t\tif len(effalle) != 1 or len(e) != 1: #remove indels\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif CK_AMB_SNP(effalle,e): #check strand-ambiguous SNPs\n",
    "\t\t\t\tstrandAmbiguousVar += 1\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif var in uniq_var:\n",
    "\t\t\t\tdupVar += 1\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\tuniq_var.add(var)\n",
    "\t\t\tout.write('\\t'.join(OUTL[0:3]+[e,OUTL[4]])+'\\n')\n",
    "\tfh.close()\n",
    "\tout.close()\n",
    "\tos.system('head -n 1 '+out_file+'.unsort >'+out_file+';tail -n +2 '+out_file+'.unsort|sort -k1,1 -k2,2n >>'+out_file+';rm -f '+out_file+'.unsort')\n",
    "\n",
    "\n",
    "#########################################\n",
    "# split SNPs into chunks of 10,000 SNPs\n",
    "########################################\n",
    "\n",
    "def SNPSPLIT(f):\n",
    "    fileDir,snpfile = os.path.split(f)\n",
    "    os.system('tail -n +2 '+f+'| split -l 10000 - '+f+'_x')\n",
    "    snpchunks = [x for x in os.listdir(fileDir) if x.startswith(snpfile+'_x') and not x.endswith('.region')]\n",
    "    for snpchunk in snpchunks:\n",
    "        snpchunkfile = os.path.join(fileDir,snpchunk)\n",
    "        os.system('head -n 1 '+f+' >'+snpchunkfile+'.tmp')\n",
    "        os.system('cat '+snpchunkfile+' >>'+snpchunkfile+'.tmp')\n",
    "        os.system('mv '+snpchunkfile+'.tmp '+snpchunkfile)\n",
    "        os.system('tail -n +2 '+snpchunkfile+'|awk \\'BEGIN{FS=OFS=\"\\t\"} {print $1,$2,$2}\\'  >'+snpchunkfile+'.region')\n",
    "    return snpchunks\n",
    "\n",
    "\n",
    "### functions for Filter for matching PGS and SJLIFE/CCSS variants\n",
    "#get region from PRS_file\n",
    "#extract SJLIFE_CCSS variant from these regions\n",
    "def VAREXT(prsf,sjccssvarf,prematchfile):\n",
    "    os.system('bgzip -d -c '+sjccssvarf+'|head -n 1 >'+prematchfile+'.tmp')\n",
    "    os.system('tabix -R '+prsf+'.region '+sjccssvarf+' >>'+prematchfile+'.tmp')\n",
    "    REFORMFILE(prematchfile)\n",
    "\n",
    "#split multiallele variants and remove indel\n",
    "def REFORMFILE(prematchfile):\n",
    "    fh = open(prematchfile+'.tmp')\n",
    "    out = open(prematchfile,'w')\n",
    "    out.write(fh.readline())\n",
    "    for line in fh:\n",
    "            l = line.strip().split('\\t')\n",
    "            #remove indel\n",
    "            if len(l[2]) != 1:\n",
    "                continue\n",
    "            for e in l[3].split(','):\n",
    "                if len(e) != 1:\n",
    "                    continue\n",
    "                out.write('\\t'.join(l[0:3]+[e])+'\\n')\n",
    "    out.close()\n",
    "    fh.close()\n",
    "\n",
    "\n",
    "#hwe extract and filter\n",
    "#take output of VAREXT as input\n",
    "def HWEEXT(prsprematchfile,hwefile,preCalhwe):\n",
    "    os.system('tail -n +2 '+prsprematchfile+'|awk \\'BEGIN{FS=OFS=\"\\t\"} {print $1,$2,$2}\\'  >'+hwefile+'.region')\n",
    "    os.system('bgzip -d -c '+preCalhwe+'|head -n 1 >'+hwefile)\n",
    "    os.system('tabix -R '+hwefile+'.region '+preCalhwe+' >>'+hwefile)\n",
    "\n",
    "\n",
    "#extract variant with hwe p vaule <= than 1e-6 and record in a file\n",
    "def HWEFILTER(hwepfile,hwepfile_filtered):\n",
    "    dfhwe = pd.read_table(hwepfile)\n",
    "    dfhwe = dfhwe[dfhwe['p'] <= 1e-6]\n",
    "    dfhwe.to_csv(hwepfile_filtered,index=False,header=True,sep='\\t')\n",
    "\n",
    "def DROPVARBYHWE(prematchVar,filteredhwe,var_filtered):\n",
    "    global hweFailedVar \n",
    "    dfprevar = pd.read_table(prematchVar)\n",
    "    dfprevar_count = dfprevar.shape[0]\n",
    "    dfhwe = pd.read_table(filteredhwe,usecols=['chr','pos','ref','alt'])\n",
    "    dfhwe.columns=['chr','pos','ref_allele','alt_allele']\n",
    "    keys = list(dfhwe.columns.values)\n",
    "    i1 = dfprevar.set_index(keys).index\n",
    "    i2 = dfhwe.set_index(keys).index\n",
    "    dfprevar = dfprevar[~i1.isin(i2)]\n",
    "    hweDrop = dfprevar_count - dfprevar.shape[0]\n",
    "    hweFailedVar += hweDrop\n",
    "    dfprevar.to_csv(var_filtered,index=False,header=True,sep='\\t')\n",
    "\n",
    "\n",
    "def VARMATCH(prematchfile,prsf,matchedfile):\n",
    "    pgs = pd.read_table(prsf,dtype=\"string\")\n",
    "    sjlife_ccss = pd.read_table(prematchfile,dtype=\"string\")\n",
    "    #forward strand\n",
    "    pgs_sjlife_ccss_merged = pgs.merge(sjlife_ccss, left_on=[\"chr_name\",\"chr_position\",\"effect_allele\",\"reference_allele\"], right_on=[\"chr\",\"pos\",\"ref_allele\",\"alt_allele\"])\n",
    "    pgs_matched = pgs_sjlife_ccss_merged.loc[:,[\"chr_name\",\"chr_position\",\"effect_allele\",\"reference_allele\",\"effect_weight\"]].to_numpy()\n",
    "    pgs_sjlife_ccss_merged = pgs.merge(sjlife_ccss, left_on=[\"chr_name\",\"chr_position\",\"reference_allele\",\"effect_allele\"], right_on=[\"chr\",\"pos\",\"ref_allele\",\"alt_allele\"])\n",
    "    pgs_matched = np.concatenate((pgs_matched, pgs_sjlife_ccss_merged.loc[:,[\"chr_name\",\"chr_position\",\"effect_allele\",\"reference_allele\",\"effect_weight\"]].to_numpy()))\n",
    "    #reverse strand\n",
    "    pgs_rev = pgs.copy()\n",
    "    pgs_effect_alleles = list(map(Seq, pgs.loc[:,\"effect_allele\"].tolist()))\n",
    "    pgs_ref_alleles = list(map(Seq, pgs.loc[:,\"reference_allele\"].tolist()))\n",
    "    pgs_effect_alleles_rev = list(map(Seq.reverse_complement, pgs_effect_alleles))\n",
    "    pgs_ref_alleles_rev = list(map(Seq.reverse_complement, pgs_ref_alleles))\n",
    "    pgs_rev.loc[:,\"effect_allele\"] = list(map(str, pgs_effect_alleles_rev))\n",
    "    pgs_rev.loc[:,\"reference_allele\"] = list(map(str, pgs_ref_alleles_rev))\n",
    "    pgs_rev = pgs_rev.astype(\"string\")\n",
    "    pgs_sjlife_ccss_merged = pgs_rev.merge(sjlife_ccss, left_on=[\"chr_name\",\"chr_position\",\"effect_allele\",\"reference_allele\"], right_on=[\"chr\",\"pos\",\"ref_allele\",\"alt_allele\"])\n",
    "    pgs_matched = np.concatenate((pgs_matched, pgs_sjlife_ccss_merged.loc[:,[\"chr_name\",\"chr_position\",\"effect_allele\",\"reference_allele\",\"effect_weight\"]].to_numpy()))\n",
    "    pgs_sjlife_ccss_merged = pgs_rev.merge(sjlife_ccss, left_on=[\"chr_name\",\"chr_position\",\"reference_allele\",\"effect_allele\"], right_on=[\"chr\",\"pos\",\"ref_allele\",\"alt_allele\"])\n",
    "    pgs_matched = np.concatenate((pgs_matched, pgs_sjlife_ccss_merged.loc[:,[\"chr_name\",\"chr_position\",\"effect_allele\",\"reference_allele\",\"effect_weight\"]].to_numpy()))\n",
    "    np.savetxt(matchedfile, pgs_matched, fmt=\"%s\", delimiter=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### PRS computation\n",
    "#make a map between sample IDs and sample names\n",
    "\n",
    "def compute_prs(snpfile):\n",
    "    prs = {x:{'score':0,'effcount':0} for x in samples}\n",
    "    # split SNPs by chr and write to temp files \n",
    "    chr2tmpsnpfile, pos2snp = parse_snpfile(snpfile)\n",
    "    print(snpfile)\n",
    "    tmpfiles = [chr2tmpsnpfile[x] for x in chr2tmpsnpfile]\n",
    "    # For each SNP chr, run bcftools and filter by maf/callrate\n",
    "    # pos2snp{'C': ['C', 'T', 0.1261920252377594], 'EAidx': ['1'], 'idx2effallele': {'1': 'C'}, 'effectallelefrequency': 0.1343669250645995, 'gtlst': ['0/0', '0/0', '0/0', '0/1', '0/0']}\n",
    "    for c in chr2tmpsnpfile:\n",
    "        tmpchrfile = chr2tmpsnpfile[c]\n",
    "        run_bcftools(c,tmpchrfile,pos2snp)\n",
    "    for v in pos2snp:\n",
    "        pos2snpvar = pos2snp[v]\n",
    "        if not 'EAidx' in pos2snpvar:\n",
    "            continue\n",
    "        radEAidx = pos2snpvar['EAidx'][0]\n",
    "        radEAnue = pos2snpvar['idx2effallele'][radEAidx]\n",
    "        radWeight = pos2snpvar[radEAnue][2]\n",
    "        for gi,gt in enumerate(pos2snpvar['gtlst']):\n",
    "            if gt == '.':\n",
    "                prs[samples[gi]]['score'] += (2 * pos2snpvar['effectallelefrequency'] * radWeight)\n",
    "                continue\n",
    "            for a in gt.split('/'):\n",
    "                if a in pos2snpvar['EAidx']:\n",
    "                    effnue = pos2snpvar['idx2effallele'][a]\n",
    "                    prs[samples[gi]]['effcount'] += 1\n",
    "                    prs[samples[gi]]['score'] += pos2snpvar[effnue][2]\n",
    "    return prs,tmpfiles\n",
    "\n",
    "\n",
    "def run_bcftools(chr,tmpsnpfile,pos2snp):\n",
    "    global mafLowVar,lowCallRateVar\n",
    "    bcfdir = '/research/rgs01/resgen/legacy/gb_customTracks/tp/files/hg38/sjlife/bcf/INFOGT/'\n",
    "    bcffile = os.path.join(bcfdir,'chr'+chr+'_SJLIFE_CCSS.GT.bcf.gz')\n",
    "    bcfCall = sp.run(\"bcftools query -R \"+tmpsnpfile+\" -f '%CHROM %POS %REF %ALT [%GT ]\\\\n' -S \"+args.sample+\".intid \"+bcffile,shell=True,stdout=sp.PIPE,stderr=sp.PIPE)\n",
    "    if not bcfCall.stdout:\n",
    "        print('bcftools query error: '+bcfCall.stderr.decode('utf-8'))\n",
    "        sys.exit(1)\n",
    "    variants = bcfCall.stdout.decode('utf-8').strip().split('\\n')\n",
    "    for v in variants:\n",
    "        l = v.strip().split(' ')\n",
    "        snp = pos2snp['.'.join([l[0],l[1]])]\n",
    "        EAidx,idx2effallele = get_effectalleidx(snp, l[2], l[3])\n",
    "        missingcallrate, effectallelefrequency,maf = get_effalefreq(l, EAidx)\n",
    "        #if '.'.join([l[0],l[1]]) == '3.178937643':\n",
    "        #    print(EAidx,idx2effallele,missingcallrate, effectallelefrequency,maf)\n",
    "        if args.maf:\n",
    "            if maf <= 0.01:\n",
    "                mafLowVar += 1\n",
    "                continue\n",
    "        if missingcallrate >= 0.1: #low callrate\n",
    "                lowCallRateVar += 1\n",
    "                continue\n",
    "        snp['EAidx'] = EAidx\n",
    "        snp['idx2effallele'] = idx2effallele\n",
    "        snp['effectallelefrequency'] = effectallelefrequency\n",
    "        snp['gtlst'] = l[4:]\n",
    "\n",
    "\n",
    "\n",
    "#return missingcallrate, \n",
    "#effectallelefrequency,\n",
    "#maf\n",
    "\n",
    "def get_effalefreq(l,EAidx):\n",
    "    EAidx = set(EAidx)\n",
    "    eacount = 0\n",
    "    total = 0\n",
    "    missing = 0\n",
    "    EAC = {} #count for each allele\n",
    "    for gt in l[4:]:\n",
    "        if gt == '.':\n",
    "            missing += 1\n",
    "            continue\n",
    "        for a in gt.split('/'):\n",
    "            if a in EAidx:\n",
    "                eacount += 1\n",
    "            if not a in EAC:\n",
    "                EAC[a] = 0\n",
    "            EAC[a] += 1\n",
    "        total += 2\n",
    "    #calculate maf\n",
    "    MAFS = {x:(EAC[x]/total) for x in EAC}\n",
    "    MAFS_sortkey = sorted(MAFS,key=lambda x: MAFS[x])\n",
    "    if len(MAFS_sortkey) > 1:\n",
    "        mafidx = MAFS_sortkey[-2]\n",
    "        maf = MAFS[mafidx]\n",
    "        for a in gt.split('/'):\n",
    "            if a in EAidx:\n",
    "                eacount += 1\n",
    "            if not a in EAC:\n",
    "                EAC[a] = 0\n",
    "            EAC[a] += 1\n",
    "        total += 2\n",
    "    #calculate maf\n",
    "    MAFS = {x:(EAC[x]/total) for x in EAC}\n",
    "    MAFS_sortkey = sorted(MAFS,key=lambda x: MAFS[x])\n",
    "    if len(MAFS_sortkey) > 1:\n",
    "        mafidx = MAFS_sortkey[-2]\n",
    "        maf = MAFS[mafidx]\n",
    "    else:\n",
    "        maf = 1\n",
    "    if total == 0:\n",
    "        print(l[0:4])\n",
    "        return [0,0,1]\n",
    "    else:\n",
    "        return [(missing/len(l[4:])), eacount/total, maf]\n",
    "\n",
    "\n",
    "def get_effectalleidx(snp,ref,alt):\n",
    "    alts = alt.split(',')\n",
    "    effalleleidx = []\n",
    "    idx2effallele = {}\n",
    "    if ref in snp:\n",
    "        effalleleidx.append('0')\n",
    "        idx2effallele['0'] = ref\n",
    "    for i,a in enumerate(alts):\n",
    "        if a in snp:\n",
    "            effalleleidx.append(str(i+1))\n",
    "            idx2effallele[str(i+1)] = a\n",
    "    return effalleleidx,idx2effallele\n",
    "\n",
    "def parse_snpfile(snpfile):\n",
    "    chr2snp = {}\n",
    "    pos2snp = {}\n",
    "    fh = open(snpfile)\n",
    "    for line in fh:\n",
    "            l = line.strip().split('\\t')\n",
    "            chr,pos,effallele,oth,weight = l\n",
    "            weight = float(weight)\n",
    "            if not chr in chr2snp:\n",
    "                chr2snp[chr] = []\n",
    "            chr2snp[chr].append('\\t'.join([chr,pos]))\n",
    "            cp = '.'.join([chr,pos])\n",
    "            if not cp in pos2snp:\n",
    "                pos2snp[cp] = {}\n",
    "            pos2snp[cp][effallele] = [effallele,oth,weight]\n",
    "    fh.close()\n",
    "    chr2tmpsnpfile = {}\n",
    "    for c in chr2snp:\n",
    "        out = open(snpfile+'.'+c,'w')\n",
    "        chr2tmpsnpfile[c] = snpfile+'.'+c\n",
    "        for ep in chr2snp[c]:\n",
    "            out.write(ep+'\\n')\n",
    "        out.close()\n",
    "    return [chr2tmpsnpfile,pos2snp]\n",
    "\n",
    "###tools\n",
    "def CKROW(f):\n",
    "    rowNum = sp.run('wc '+f,shell=True,stdout=sp.PIPE).stdout.decode('utf-8').strip().split(' ')[0]\n",
    "    if int(rowNum) <= 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def OUTPUTSUM(sexChromosomeVar,strandAmbiguousVar,dupVar,lowCallRateVar,hweFailedVar,mafLowVar,outf):\n",
    "    outk = ['sexChromosomeVar','strandAmbiguousVar','dupVar','lowCallRateVar','hweFailedVar','mafLowVar']\n",
    "    outv = [sexChromosomeVar,strandAmbiguousVar,dupVar,lowCallRateVar,hweFailedVar,mafLowVar]\n",
    "    out = open(outf,'w')\n",
    "    for i,e in enumerate(outk):\n",
    "        out.write('\\t'.join([e,str(outv[i])])+'\\n')\n",
    "    out.close()\n",
    "\n",
    "import os,re\n",
    "import gzip\n",
    "import argparse\n",
    "from argparse import RawTextHelpFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio.Seq import Seq\n",
    "import subprocess as sp\n",
    "import sys\n",
    "\n",
    "parser = argparse.ArgumentParser(description=script_description,formatter_class=RawTextHelpFormatter)\n",
    "parser.add_argument('-s','--score',help='PGS score file downloaded from PGS catalog (PGS001238_hmPOS_GRCh38.txt.gz)')\n",
    "parser.add_argument('-d','--workingDir',help='Working directory, final prs score file will be generated under here')\n",
    "parser.add_argument('--sample',help='sample file')\n",
    "parser.add_argument('--maf',help='maf filter. default: off',action=\"store_true\")\n",
    "parser.add_argument('--hwe',help='hwe file')\n",
    "parser.add_argument('--ancestry',help='ancestry: asa,ceu or yri')\n",
    "parser.add_argument('-o','--output',help='output file')\n",
    "parser.add_argument('--statout',help='summary output')\n",
    "parser.add_argument('-v','--var',help='SJLIFE_CCSS variants')\n",
    "parser.add_argument('--matchout',help='output matched variant list. default: off',action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "#maf filter\n",
    "if args.maf:\n",
    "    mafMark = '.maf'\n",
    "else:\n",
    "    mafMark = ''\n",
    "\n",
    "##############################\n",
    "# Initial QC of variant data\n",
    "##############################\n",
    "# Discard the following variants: duplicated variants, non-SNP variants (e.g. indels), non-autosomal SNPs, and strand-ambiguous SNPs\n",
    "PGSID = os.path.split(args.score)[-1].split('_')[0]\n",
    "qc_PRS_file = os.path.join(args.workingDir,PGSID+'.'+args.ancestry+mafMark)\n",
    "sexChromosomeVar = 0 #remove variants on sex chromosome\n",
    "strandAmbiguousVar = 0 #remove ambiguous variants\n",
    "dupVar = 0 #remove duplicated variants\n",
    "QCVAR(args.score,qc_PRS_file)\n",
    "#more QCs:\n",
    "lowCallRateVar = 0\n",
    "hweFailedVar = 0\n",
    "mafLowVar = 0\n",
    "#check score file row number\n",
    "rowCK = CKROW(qc_PRS_file)\n",
    "if not rowCK:\n",
    "    print('There are no variants: '+qc_PRS_file)\n",
    "    print('Skip calculating PRS for '+PGSID+'('+args.ancestry+')')\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "#########################################\n",
    "# split SNPs into chunks of 10,000 SNPs\n",
    "########################################\n",
    "SNP_CHUNKS = SNPSPLIT(qc_PRS_file)\n",
    "\n",
    "\n",
    "matchedVariantFile = os.path.join(args.workingDir,'sjlife.ccss.variants.Match')\n",
    "if args.matchout:\n",
    "\tgenMatch = True\n",
    "else:\n",
    "\tgenMatch = False\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "# PRS computation\n",
    "#############################\n",
    "\n",
    "#sampleID to integer ID\n",
    "samples = [x.strip() for x in open(args.sample)]\n",
    "SJLIFE_CCSS_varfile = args.var\n",
    "PRS = {x:{'score':0,'effcount':0} for x in samples}\n",
    "for s in SNP_CHUNKS:\n",
    "    snpchunk = os.path.join(args.workingDir,s)\n",
    "    newGenFiles = [snpchunk,snpchunk+'.region']\n",
    "    prematch_var = os.path.join(args.workingDir,'sjlife.ccss.variants.'+args.ancestry+mafMark+'.preMatch.txt') #sjlife.ccss prematch file\n",
    "    newGenFiles.extend([prematch_var,prematch_var+'.tmp'])\n",
    "    VAREXT(snpchunk,SJLIFE_CCSS_varfile,prematch_var) #extract variants from SJLIFE_CCSS DATA, generate prematch_var\n",
    "    if genMatch:\n",
    "        VARMATCH(prematch_var,snpchunk,matchedVariantFile+'.tmp')\n",
    "        os.system('cat '+matchedVariantFile+'.tmp >>'+matchedVariantFile)\n",
    "        os.system('rm -f '+matchedVariantFile+'.tmp')\n",
    "    #remove variants with hwe p value less than 1e-6\n",
    "    var_hwe = os.path.join(args.workingDir,'sjlife.ccss.variants.'+args.ancestry+mafMark+'.preMatch.txt.hwe')\n",
    "    newGenFiles.extend([var_hwe,var_hwe+'.region'])\n",
    "    HWEEXT(prematch_var,var_hwe,args.hwe) #generate hwe p valur for sjlife_ccss variant\n",
    "    var_hwe_filtered = var_hwe+'.filtered'\n",
    "    newGenFiles.append(var_hwe_filtered)\n",
    "    HWEFILTER(var_hwe,var_hwe_filtered) #generate variant with p value <= 1e-6\n",
    "    #filter sjlife_ccss variant based on hwe p value (remove var with p value <= 1e-6)\n",
    "    prematch_var_filtered = prematch_var+'_hwefiltered'\n",
    "    newGenFiles.append(prematch_var_filtered)\n",
    "    DROPVARBYHWE(prematch_var,var_hwe_filtered,prematch_var_filtered)\n",
    "    #variant matching\n",
    "    matched_vaf_file = os.path.join(args.workingDir,'sjlife.ccss.variants.'+s+mafMark+'.Match') #sjlife.ccss matched file\n",
    "    #newGenFiles.append(matched_vaf_file)\n",
    "    VARMATCH(prematch_var_filtered,snpchunk,matched_vaf_file)\n",
    "    prs_snpchunk,tmpFiles = compute_prs(matched_vaf_file)\n",
    "    newGenFiles.append(matched_vaf_file)\n",
    "    newGenFiles.extend(tmpFiles)\n",
    "    for sam in prs_snpchunk:\n",
    "        PRS[sam]['score'] += prs_snpchunk[sam]['score']\n",
    "        PRS[sam]['effcount'] += prs_snpchunk[sam]['effcount']\n",
    "    for delfile in newGenFiles:\n",
    "        os.system('rm -f '+delfile)\n",
    "    print(s+' done!')\n",
    "\n",
    "\n",
    "#output the total RPS data as a tab-delimited table\n",
    "out = open(args.output,'w')\n",
    "out.write('\\t'.join(['sampleName','effcount','score'])+'\\n')\n",
    "\n",
    "for s in PRS:\n",
    "    out.write('\\t'.join([s,str(PRS[s]['effcount']),str(PRS[s]['score'])])+'\\n')\n",
    "out.close()\n",
    "#output summary file\n",
    "OUTPUTSUM(sexChromosomeVar,strandAmbiguousVar,dupVar,lowCallRateVar,hweFailedVar,mafLowVar,args.statout)\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e817b1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gentracktable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gentracktable.py\n",
    "\n",
    "#!/usr/bin/python3\n",
    "\n",
    "script_description=\"\"\"\n",
    "generate track_table.prs\n",
    "\tinput files:\n",
    "\t\t\tpgs_all_metadata.xlsx Download from https://www.pgscatalog.org/downloads/\n",
    "\t\t\tpgs_traits_data.json Download from https://www.pgscatalog.org/browse/traits/\n",
    "           \n",
    "                        \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "EAS: Asian\n",
    "EUR: European\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import re,sys\n",
    "import argparse\n",
    "from argparse import RawTextHelpFormatter\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import subprocess as sp\n",
    "\n",
    "def GETGWASANC(pgs):\n",
    "\tlink = 'https://www.pgscatalog.org/rest/score/'+pgs\n",
    "\tf = requests.get(link)\n",
    "\tfjs = json.loads(f.text)\n",
    "\tif not 'ancestry_distribution' in fjs:\n",
    "\t\twhile True:\n",
    "\t\t\ttime.sleep(10)\n",
    "\t\t\tf = requests.get(link)\n",
    "\t\t\tfjs = json.loads(f.text)\n",
    "\t\t\tif 'ancestry_distribution' in fjs:\n",
    "\t\t\t\tbreak\n",
    "\tancestry_distribution = fjs['ancestry_distribution']\n",
    "\tif 'gwas' in ancestry_distribution:\n",
    "\t\tgwas_ancestry = ancestry_distribution['gwas']['dist']\n",
    "\telif 'dev' in ancestry_distribution:\n",
    "\t\tgwas_ancestry = ancestry_distribution['dev']['dist']\n",
    "\telse:\n",
    "\t\tgwas_ancestry = 'Mixed'\n",
    "\treturn gwas_ancestry\n",
    "\n",
    "def PARSEPOP(gwas_ancestry,pop):\n",
    "\tSELECTPOP = {'Asian','European','African'}\n",
    "\tif gwas_ancestry == 'Mixed':\n",
    "\t\treturn False\n",
    "\tgwas_ancestry = list({pop[x] for x in gwas_ancestry})\n",
    "\tif len(gwas_ancestry) == 1 and gwas_ancestry[0] in SELECTPOP:\n",
    "\t\treturn gwas_ancestry[0]+' ancestry'\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=script_description,formatter_class=RawTextHelpFormatter)\n",
    "parser.add_argument('-m','--meta',help='pgs_all_metadata.xlsx')\n",
    "parser.add_argument('-t','--traits',help='pgs_traits_data.json')\n",
    "parser.add_argument('-w','--workingdir',help='working directory where all PGS folders can be found')\n",
    "parser.add_argument('-o','--output',help='file include PGS scores')\n",
    "parser.add_argument('-p','--path',help='path to prs scores')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "POP = {'SAS':'Asian', 'EUR':'European', 'AFR':'African', 'MAO':'Mixed','EAS':'Asian', 'AMR':'Mixed', 'OTH':'Mixed', 'NR':'Mixed', 'ASN':'Asian','MAE':'Mixed','GME':'Mixed'}\n",
    "\n",
    "\n",
    "#pgs_metadata_table\n",
    "pgs_metadata_table = pd.read_excel(args.meta,sheet_name=\"Scores\")\n",
    "\n",
    "#pgs_traits {key:Trait identifier,value:[trait,parent]}\n",
    "#randomally select one super parent if there are more than one appendicitis in Digestive system disorder&Immune system disorder\n",
    "pgs_traits_data = json.loads(open(args.traits).read())\n",
    "th = pgs_traits_data['header'][0] #table_header\n",
    "pgs_traits = {}\n",
    "#four traits are overlapped with terms from Kyla\n",
    "#add \"(PRS)\" for these 4 traits\n",
    "dup_traits = {'Cholecystitis','Hypercholesterolemia','Hematuria','Hyperthyroidism'} \n",
    "for t in pgs_traits_data['data']:\n",
    "\ttid = t[th[1]].strip().split(':')[0]\n",
    "\tparent = re.findall('[A-Z][^A-Z]*',t[th[2]])\n",
    "\tparent = parent[0]\n",
    "\ttrait = t[th[0]]\n",
    "\tif trait in dup_traits:\n",
    "\t\ttrait += ' (PRS)'\n",
    "\tpgs_traits[tid] = [trait,parent]\n",
    "\n",
    "#pgs2traitParent\n",
    "pgs2traitParent = {}\n",
    "for index,row in pgs_metadata_table.iterrows():\n",
    "\tpgs2traitParent[row['Polygenic Score (PGS) ID']] = pgs_traits[row['Mapped Trait(s) (EFO ID)'].split('|')[0]] #randomally select one parent if there are more than one\n",
    "\n",
    "\n",
    "PGSs = [x for x in os.listdir(args.workingdir) if x.startswith('PGS') and os.path.isdir(os.path.join(args.workingdir,x))]\n",
    "\n",
    "out = open(args.output,'w')\n",
    "out.write('\\t'.join(['prs','sample_ancestry','prs_name','ancestry','tp_path'])+'\\n')\n",
    "for pgs in PGSs:\n",
    "\tnumOfMatVar = sp.run('wc '+os.path.join(args.workingdir,pgs+'/sjlife.ccss.variants.Match'),shell=True,stdout=sp.PIPE).stdout.decode('utf-8').strip().split(' ')[0] \n",
    "\tif not numOfMatVar or int(numOfMatVar) < 5:\n",
    "\t\tcontinue\n",
    "\ttrack_table_ancestry = ','.join(['Genetic Factors','Polygenic Risk Scores',pgs2traitParent[pgs][1],pgs2traitParent[pgs][0]])\n",
    "\tprs_outancestry = ['All ancestries']\n",
    "\tpgs_gwas_ancestry = GETGWASANC(pgs)\n",
    "\tselpop = PARSEPOP(pgs_gwas_ancestry,POP)\n",
    "\tif selpop:\n",
    "\t\tprs_outancestry.append(selpop)\n",
    "\t\tderivFrom = 'Derived from '+selpop\n",
    "\telse:\n",
    "\t\tderivFrom = 'Derived from Multi-ancestry'\n",
    "\tfor a in prs_outancestry:\n",
    "\t\tprs = ' '.join([pgs,pgs2traitParent[pgs][0],'('+'; '.join([derivFrom,'Computed for '+a])+')'])\n",
    "\t\tout.write('\\t'.join([pgs,a,prs,track_table_ancestry,os.path.join(args.path,pgs)])+'\\n')\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88034ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
